// NOMA TTA - TRUE Causal Test-Time Adaptation with LoRA
// ============================================================
// This is a PROPER TTA implementation with causal predictions:
// For each batch, we PREDICT FIRST, then ADAPT.
// This ensures each prediction uses the model state BEFORE seeing that batch.
//
// Protocol:
// 1. Load frozen backbone weights
// 2. Evaluate clean phase with frozen backbone (no LoRA)
// 3. At t0 (drift detection), inject LoRA adapter on last layer
// 4. For each drift batch:
//    a) PREDICT with current model (causal prediction)
//    b) ADAPT model using labels (online update)
// 5. Save predictions from each step
//
// LoRA Implementation:
// Original: y = xW2 + b2
// With LoRA: y = xW2 + x(A*B) + b2  where A:[in,rank], B:[rank,out]
// Initialized to zero: A*B = 0, so output unchanged at injection

fn main() {
    let optimizer = 2.0;       // Adam for online updates
    let learning_rate = 0.01;  // Higher LR for fast adaptation
    let beta1 = 0.9;
    let beta2 = 0.999;
    let epsilon = 0.00000001;

    print("=== NOMA TTA - CAUSAL Online Adaptation with LoRA ===");
    print("Mode: Predict-then-Adapt (true streaming TTA)");
    
    // =========================================================================
    // Load Frozen Backbone Weights (these will NOT be updated)
    // =========================================================================
    print("Loading frozen backbone weights...");
    load_safetensors_named W1 = "noma_TTA/output/backbone_weights.safetensors" : "W1";
    load_safetensors_named b1 = "noma_TTA/output/backbone_weights.safetensors" : "b1";
    load_safetensors_named W2 = "noma_TTA/output/backbone_weights.safetensors" : "W2";
    load_safetensors_named b2 = "noma_TTA/output/backbone_weights.safetensors" : "b2";
    
    // Store original backbone checksum for verification
    let backbone_checksum = sum(W1 * W1) + sum(W2 * W2);
    print("Backbone loaded. Initial checksum:");
    print(backbone_checksum);
    
    // =========================================================================
    // Initialize LoRA Adapter for Last Layer (W2: 128 -> 10)
    // LoRA: delta_W = A @ B where A:[128, rank], B:[rank, 10]
    // Initialize A to small random, B to zeros -> A@B = 0 (neutral injection)
    // =========================================================================
    
    learn lora_A = rand_tensor(128.0, 4.0) * 0.01;  // [128, 4]
    learn lora_B = rand_tensor(4.0, 10.0) * 0.0;    // [4, 10] = zeros
    
    print("LoRA adapter initialized (rank=4, neutral injection).");
    
    // =========================================================================
    // Load Phase Data Separately
    // =========================================================================
    print("Loading streaming data...");
    
    load_safetensors_named X_clean = "noma_TTA/data/stream_clean.safetensors" : "x";
    load_safetensors_named Y_clean_labels = "noma_TTA/data/stream_clean.safetensors" : "y";
    load_safetensors_named T_clean = "noma_TTA/data/stream_clean.safetensors" : "t";
    load_safetensors_named Phase_clean = "noma_TTA/data/stream_clean.safetensors" : "phase";
    load_safetensors_named Intensity_clean = "noma_TTA/data/stream_clean.safetensors" : "intensity";
    
    load_safetensors_named X_drift = "noma_TTA/data/stream_drift.safetensors" : "x";
    load_safetensors_named Y_drift = "noma_TTA/data/stream_drift.safetensors" : "y_onehot";
    load_safetensors_named Y_drift_labels = "noma_TTA/data/stream_drift.safetensors" : "y";
    load_safetensors_named T_drift = "noma_TTA/data/stream_drift.safetensors" : "t";
    load_safetensors_named Phase_drift = "noma_TTA/data/stream_drift.safetensors" : "phase";
    load_safetensors_named Intensity_drift = "noma_TTA/data/stream_drift.safetensors" : "intensity";
    
    print("Stream loaded: 30000 clean + 30000 drift samples");
    
    // =========================================================================
    // Phase 1: Clean Phase - Evaluate with frozen backbone (NO LoRA)
    // =========================================================================
    print("Phase 1: Evaluating clean data with frozen backbone...");
    
    let z1_clean = matmul(X_clean, W1) + b1;
    let h1_clean = relu(z1_clean);
    let z2_clean = matmul(h1_clean, W2) + b2;
    let pred_clean = softmax(z2_clean);
    
    print("Clean phase evaluation complete (frozen backbone, no LoRA).");
    
    // =========================================================================
    // Phase 2: Drift Phase with TRUE CAUSAL TTA
    // Using streaming_adapt: for each batch, PREDICT then ADAPT
    // =========================================================================
    print("Phase 2: Drift detected! Starting CAUSAL TTA adaptation...");
    print("Injection point: t = 30000");
    print("Protocol: For each batch -> Predict FIRST, then Adapt");
    
    let batch_size = 512.0;
    
    // The streaming_adapt construct ensures:
    // 1. Predict block runs FIRST (stores predictions)
    // 2. Adapt block runs AFTER (updates only learn parameters)
    // This gives us TRUE causal TTA predictions
    
    streaming_adapt X_drift, Y_drift with batch_size -> x_batch, y_batch
        predict {
            // Forward pass with current LoRA state (BEFORE update)
            let z1 = matmul(x_batch, W1) + b1;
            let h1 = relu(z1);
            let lora_delta = matmul(lora_A, lora_B);
            let z2 = matmul(h1, W2) + matmul(h1, lora_delta) + b2;
            let pred = softmax(z2);
        } -> drift_preds
        adapt {
            // Forward pass for loss (same computation, different purpose)
            let z1 = matmul(x_batch, W1) + b1;
            let h1 = relu(z1);
            let lora_delta = matmul(lora_A, lora_B);
            let z2 = matmul(h1, W2) + matmul(h1, lora_delta) + b2;
            let pred = softmax(z2);
            
            // Supervised loss for online update
            let error = pred - y_batch;
            let loss = mean(error * error);
            
            // Only LoRA parameters are 'learn', so only they update
            minimize loss;
        }
    
    print("Causal TTA adaptation complete.");
    print("Predictions collected BEFORE each adaptation step.");
    
    // =========================================================================
    // Verify Backbone Integrity
    // =========================================================================
    let backbone_checksum_final = sum(W1 * W1) + sum(W2 * W2);
    let checksum_diff = abs(backbone_checksum_final - backbone_checksum);
    
    print("Backbone checksum verification:");
    print("  Initial checksum:");
    print(backbone_checksum);
    print("  Final checksum:");
    print(backbone_checksum_final);
    print("  Difference (should be 0):");
    print(checksum_diff);
    
    // =========================================================================
    // Save Results - Separate files for clean and drift phases
    // =========================================================================
    print("Saving CAUSAL TTA results...");
    
    // Save clean phase (evaluated with frozen backbone, no LoRA)
    save_safetensors {
        pred_probs: pred_clean,
        y_true: Y_clean_labels,
        t: T_clean,
        phase: Phase_clean,
        intensity: Intensity_clean
    }, "noma_TTA/output/eval_tta_causal_clean.safetensors";
    
    // Save drift phase (CAUSAL predictions, collected BEFORE adaptation)
    save_safetensors {
        pred_probs: drift_preds,
        y_true: Y_drift_labels,
        t: T_drift,
        phase: Phase_drift,
        intensity: Intensity_drift,
        lora_A: lora_A,
        lora_B: lora_B,
        backbone_checksum_initial: backbone_checksum,
        backbone_checksum_final: backbone_checksum_final
    }, "noma_TTA/output/eval_tta_causal_drift.safetensors";
    
    print("Results saved to:");
    print("  - noma_TTA/output/eval_tta_causal_clean.safetensors (frozen backbone)");
    print("  - noma_TTA/output/eval_tta_causal_drift.safetensors (causal TTA preds)");
    print("=== CAUSAL TTA Evaluation Complete ===");
    
    return 0.0;
}

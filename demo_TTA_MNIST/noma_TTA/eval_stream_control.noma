// NOMA TTA - Control Experiment (LoRA Injected, No Updates)
// Injects LoRA adapter but does NOT perform online updates
// This verifies that performance gains come from learning, not architecture
//
// Expected outcome: Performance should match baseline (no improvement)
// because LoRA is initialized to zero and never updated

fn main() {
    print("=== NOMA TTA - Control (LoRA Injected, No Updates) ===");
    
    // =========================================================================
    // Load Frozen Backbone Weights
    // =========================================================================
    print("Loading frozen backbone weights...");
    load_safetensors_named W1 = "noma_TTA/output/backbone_weights.safetensors" : "W1";
    load_safetensors_named b1 = "noma_TTA/output/backbone_weights.safetensors" : "b1";
    load_safetensors_named W2 = "noma_TTA/output/backbone_weights.safetensors" : "W2";
    load_safetensors_named b2 = "noma_TTA/output/backbone_weights.safetensors" : "b2";
    print("Backbone loaded (784 -> 128 -> 10).");
    
    // =========================================================================
    // Initialize LoRA Adapter (same as TTA, but will NOT be updated)
    // =========================================================================
    let lora_rank = 4.0;
    
    // LoRA matrices initialized to produce zero output
    // A: small random, B: zeros -> A@B = 0
    let lora_A = rand_tensor(128.0, 4.0) * 0.01;  // [128, 4]
    let lora_B = rand_tensor(4.0, 10.0) * 0.0;    // [4, 10] = zeros
    
    print("LoRA adapter initialized (rank=4, zero output).");
    print("Note: LoRA will NOT be updated (control experiment).");
    
    // =========================================================================
    // Load Streaming Data
    // =========================================================================
    print("Loading streaming data...");
    load_safetensors_named X_stream = "noma_TTA/data/mnist_stream_full.safetensors" : "x";
    load_safetensors_named Y_stream = "noma_TTA/data/mnist_stream_full.safetensors" : "y";
    load_safetensors_named T_stream = "noma_TTA/data/mnist_stream_full.safetensors" : "t";
    load_safetensors_named Phase_stream = "noma_TTA/data/mnist_stream_full.safetensors" : "phase";
    load_safetensors_named Intensity_stream = "noma_TTA/data/mnist_stream_full.safetensors" : "intensity";
    
    print("Stream loaded: 60000 samples");
    print("  t0 = 30000 (distribution shift point)");
    
    // =========================================================================
    // Full Evaluation with LoRA (but zero delta, so equivalent to baseline)
    // =========================================================================
    print("Running control evaluation (LoRA injected, no updates)...");
    
    // Forward pass with LoRA (which is zero, so output identical to baseline)
    let z1 = matmul(X_stream, W1) + b1;
    let h1 = relu(z1);
    
    // Layer 2 with LoRA (A@B = 0 since B = 0)
    let lora_delta = matmul(lora_A, lora_B);  // Should be all zeros
    let z2 = matmul(h1, W2) + matmul(h1, lora_delta) + b2;
    let pred_probs = softmax(z2);
    
    print("Control evaluation complete.");
    
    // Verify LoRA is indeed zero
    let lora_norm = sum(lora_delta * lora_delta);
    print("LoRA delta norm (should be ~0):");
    print(lora_norm);
    
    // =========================================================================
    // Save Results
    // =========================================================================
    print("Saving control results...");
    save_safetensors {
        pred_probs: pred_probs,
        y_true: Y_stream,
        t: T_stream,
        phase: Phase_stream,
        intensity: Intensity_stream,
        lora_A: lora_A,
        lora_B: lora_B,
        lora_delta_norm: lora_norm
    }, "noma_TTA/output/eval_control_results.safetensors";
    
    print("Results saved to: noma_TTA/output/eval_control_results.safetensors");
    print("=== Control Evaluation Complete ===");
    
    return 0.0;
}
